{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2Vys1gLFld8CdqdGjUZ4b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/tranformer-language-translator/blob/main/Eng_%3E_Tel_Transformer_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "source: https://github.com/ajhalthor/Transformer-Neural-Network"
      ],
      "metadata": {
        "id": "poRbzkwnWJQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, ReLU, LayerNormalization\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def get_device():\n",
        "    return \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = tf.shape(q)[-1]\n",
        "    scaled = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(d_k, tf.float32))\n",
        "    if mask is not None:\n",
        "       scaled += mask  # TensorFlow supports broadcasting, so no need for permutation.\n",
        "    attention = tf.nn.softmax(scaled, axis=-1)\n",
        "    values = tf.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def call(self):\n",
        "        even_i = tf.range(0, self.d_model, 2, dtype=tf.float32)\n",
        "        denominator = tf.pow(10000.0, even_i / self.d_model)\n",
        "        position = tf.reshape(tf.range(self.max_sequence_length, dtype=tf.float32), (self.max_sequence_length, 1))\n",
        "        even_PE = tf.sin(position / denominator)\n",
        "        odd_PE = tf.cos(position / denominator)\n",
        "        stacked = tf.stack([even_PE, odd_PE], axis=2)\n",
        "        PE = tf.reshape(stacked, (self.max_sequence_length, self.d_model))\n",
        "        return PE\n",
        "\n",
        "class SentenceEmbedding(tf.keras.layers.Layer):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super(SentenceEmbedding, self).__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indices = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "               sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "               sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indices), self.max_sequence_length):\n",
        "                sentence_word_indices.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            return tf.convert_to_tensor(sentence_word_indices, dtype=tf.int32)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "            tokenized.append(tokenize(batch[sentence_num], start_token, end_token))\n",
        "        tokenized = tf.stack(tokenized)\n",
        "        return tf.identity(tokenized)\n",
        "\n",
        "    def forward(self, x, start_token, end_token):  # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder()\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = Dense(3 * d_model)\n",
        "        self.linear_layer = Dense(d_model)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        batch_size, sequence_length, d_model = tf.shape(x)\n",
        "        qkv = self.qkv_layer(x)  # (batch_size, sequence_length, 3 * d_model)\n",
        "        qkv = tf.reshape(qkv, (batch_size, sequence_length, self.num_heads, 3 * self.head_dim))\n",
        "        qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])  # (batch_size, num_heads, sequence_length, 3 * head_dim)\n",
        "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
        "        values, attention = self.scaled_dot_product(q, k, v, mask)\n",
        "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
        "        values = tf.reshape(values, (batch_size, sequence_length, self.num_heads * self.head_dim))\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "class LayerNormalization(Layer):\n",
        "    def __init__(self, parameters_shape, epsilon=1e-5, **kwargs):\n",
        "        super(LayerNormalization, self).__init__(**kwargs)\n",
        "        self.parameters_shape = parameters_shape\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = self.add_weight(shape=parameters_shape, initializer='ones', trainable=True )\n",
        "        self.beta = self.add_weight( shape=parameters_shape, initializer='zeros', trainable=True )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Compute mean and variance\n",
        "        mean, variance = tf.nn.moments(inputs, axes=list(range(len(inputs.shape) - len(self.parameters_shape))), keepdims=True)\n",
        "        # Compute standard deviation\n",
        "        std = tf.sqrt(variance + self.epsilon)\n",
        "        # Normalize\n",
        "        y = (inputs - mean) / std\n",
        "        # Scale and shift\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.dense1 = Dense(hidden, activation=None)  # No activation here, it's applied separately\n",
        "        self.relu = ReLU()\n",
        "        self.dropout = Dropout(rate=drop_prob)\n",
        "        self.dense2 = Dense(d_model, activation=None)  # No activation here\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.dense1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6, parameters_shape=(d_model,))\n",
        "        self.dropout1 = Dropout(rate=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6, parameters_shape=(d_model,))\n",
        "        self.dropout2 = Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, self_attention_mask):\n",
        "        residual_x = x\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "class SequentialEncoder(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        x, self_attention_mask = inputs\n",
        "        for layer in self._layers:\n",
        "            x = layer([x, self_attention_mask])\n",
        "        return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder()\n",
        "        self.layers._layers = [EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, x, self_attention_mask, start_token, end_token):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers([x, self_attention_mask])\n",
        "        return x\n",
        "\n",
        "class MultiHeadCrossAttention(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadCrossAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.kv_layer = Dense(2 * d_model)\n",
        "        self.q_layer = Dense(d_model)\n",
        "        self.linear_layer = Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, x, y, mask=None):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "\n",
        "        kv = self.split_heads(kv, batch_size)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "\n",
        "        k, v = tf.split(kv, num_or_size_splits=2, axis=-1)\n",
        "\n",
        "        values, attention = self.scaled_dot_product(q, k, v, mask)\n",
        "\n",
        "        values = tf.transpose(values, perm=[0, 2, 1, 3])\n",
        "        values = tf.reshape(values, (batch_size, -1, self.d_model))\n",
        "\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "class DecoderLayer(Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm1 = LayerNormalization(epsilon=1e-6, parameters_shape=(d_model,))\n",
        "        self.dropout1 = Dropout(rate=drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(epsilon=1e-6, parameters_shape=(d_model,))\n",
        "        self.dropout2 = Dropout(rate=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(epsilon=1e-6, parameters_shape=(d_model,))\n",
        "        self.dropout3 = Dropout(rate=drop_prob)\n",
        "\n",
        "    def call(self, x, y, self_attention_mask, cross_attention_mask, training=False):\n",
        "        _y = tf.identity(y)\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y, training=training)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = tf.identity(y)\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y, training=training)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = tf.identity(y)\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y, training=training)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y\n",
        "\n",
        "class SequentialDecoder(tf.keras.Model):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.seq_decoder_layers = layers\n",
        "\n",
        "    def call(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        for layer in self.seq_decoder_layers:\n",
        "            y = layer(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder_layers = SequentialDecoder([DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def call(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.decoder_layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "#this will generate the translation letter by letter not word by word\n",
        "class Transformer(Model):\n",
        "  #all architecture params are used to create an transformer object of this class\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 kn_vocab_size,\n",
        "                 english_to_index,\n",
        "                 telugu_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        #building a encoder\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        #building a decoder\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, telugu_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        #creating ffn with same number of nodes as telugu vocab size\n",
        "        self.linear = Dense(kn_vocab_size)\n",
        "\n",
        "    def call(self,\n",
        "             x,  #batch of english sentences\n",
        "             y,  #batch of telugu sentences\n",
        "             encoder_self_attention_mask=None,\n",
        "             decoder_self_attention_mask=None,\n",
        "             decoder_cross_attention_mask=None,\n",
        "             enc_start_token=False,\n",
        "             enc_end_token=False,\n",
        "             dec_start_token=False,\n",
        "             dec_end_token=False):\n",
        "      # processing x with encoder to transformer object\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "       # combine outout of encoder and process it with decoder and we push telugu directly through decoder it will tokenize and embed the telugu and also compare them\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        #decoder output push through ffn\n",
        "        out = self.linear(out)\n",
        "        #output of ffn\n",
        "        return out"
      ],
      "metadata": {
        "id": "H8EZHv8h5Hoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Traning code starts from here"
      ],
      "metadata": {
        "id": "HHm_Xx2GCMHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "english_file = '/content/drive/MyDrive/transformer_dataset/train.en' # path of english file\n",
        "telugu_file = '/content/drive/MyDrive/transformer_dataset/train.te' # path of telugu file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh0r4RIXz1Vx",
        "outputId": "92f15caf-205f-45cd-ad51-dc4755f1000d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generated this by filtering Appendix code\n",
        "\n",
        "START_TOKEN = ''\n",
        "PADDING_TOKEN = ''\n",
        "END_TOKEN = ''\n",
        "\n",
        "#source -> https://en.wikipedia.org/wiki/Telugu_(Unicode_block)\n",
        "telugu_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ',\n",
        "                   'ఀ','ఄ','౿',\t'ఁ',\t'ం',\t'ః',\n",
        "                     'అ',\t'ఆ',\t'ఇ',\t'ఈ',\t'ఉ',\t'ఊ',\t'ఋ',\t'ఌ',\t'ఎ', 'ఏ',\n",
        "                     'ఐ',\t'ఒ',\t'ఓ',\t'ఔ',\n",
        "                     'క',\t'ఖ',\t'గ',\t'ఘ',\t'ఙ',\n",
        "                     'చ',\t'ఛ',\t'జ',\t'ఝ',\t'ఞ',\n",
        "                     'ట',\t'ఠ',\t'డ',\t'ఢ',\t'ణ',\n",
        "                     'త',\t'థ',\t'ద',\t'ధ',\t'న',\n",
        "                     'ప',\t'ఫ',\t'బ',\t'భ',\t'మ',\n",
        "                     'య',\t'ర',\t'ఱ',\t'ల',\t'ళ',\t'ఴ',\n",
        "                     'వ',\t'శ',\t'ష',\t'స',\t'హ',\n",
        "                     'ఽ',\t'ాా',\t'ిి',\t'ీీ',\t'ు',\t'ూ','ృ','ౄ',\n",
        "                     'ెె',\t'ేే',\t'ైైై','ొొ','ో','  ౌ',\t'్', '్స', '్ణ', '్హ',\n",
        "                     'ౕౕ',\t'ౖౖ','ౘ',\t'ౙ',\t'ౚ',\t'ౠ',\t'ౡ',\n",
        "                     'ౢౢ',\t'ౣౣ',\t'౦',\t'౧',\t'౨',\t'౩',\t'౪',\n",
        "                     '౫',\t'౬',\t'౭',\t'౮',\t'౯', PADDING_TOKEN, END_TOKEN]\n",
        "                     #copy paste more telugu lipi from wikipedia\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@', '[',\n",
        "                        ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
      ],
      "metadata": {
        "id": "-WfOaEKR5ciz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'క' + 'ిి'"
      ],
      "metadata": {
        "id": "nSQOqYT3H8R1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "babef248-cd50-40f4-c387-399dcc37a36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'కిి'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_telugu = {k:v for k,v in enumerate(telugu_vocabulary)}\n",
        "telugu_to_index = {v:k for k,v in enumerate(telugu_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ],
      "metadata": {
        "id": "EthEpEy3IKQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_lines_from_file(file_path, total_sentences):\n",
        "    dataset = tf.data.TextLineDataset(file_path)\n",
        "    dataset = dataset.take(total_sentences)\n",
        "    return [line.numpy().decode('utf-8').strip() for line in dataset]\n",
        "\n",
        "TOTAL_SENTENCES = 10000 # Limit Number of sentences to scan from file\n",
        "                #can be increase upto 2 million\n",
        "# Read and process English sentences\n",
        "english_sentences = read_lines_from_file(english_file, TOTAL_SENTENCES)  #scanning all columns and desired number of rows\n",
        "english_sentences = [sentence.lower() for sentence in english_sentences]\n",
        "\n",
        "# Read and process telugu sentences\n",
        "telugu_sentences = read_lines_from_file(telugu_file, TOTAL_SENTENCES)  #scanning all columns and desired number of rows"
      ],
      "metadata": {
        "id": "4QIO86mIILQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences[:10] #all cols and first 10 rows i..e, first 10 english sentences from dataset"
      ],
      "metadata": {
        "id": "_xfQQnBfIe4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b473bfc0-91bd-4a17-ecd8-a5afc51a60fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rise again.',\n",
              " 'how do we glorify jehovahs undeserved kindness?',\n",
              " 'india also continues to push back economically.',\n",
              " 'i remember my childhood days.',\n",
              " 'all transactions are made online.',\n",
              " 'i love night shoots in my city.',\n",
              " 'three members of a family were killed in the incident.',\n",
              " 'the film is directed by v v vinayak.',\n",
              " 'the hyderabad weather office forecast more rains.',\n",
              " 'we are family!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "telugu_sentences[:10] #all cols and first 10 rows i..e, first 10 telugu sentences from dataset"
      ],
      "metadata": {
        "id": "gBK_6q8EIrZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8179bd-d117-4b54-8046-3f77101a4c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['మళ్లీ ఉదయిస్తాడు.',\n",
              " 'యెహోవా కృపను మనమెలా మహిమపరచవచ్చు?',\n",
              " 'ఆర్థికంగా కూడా భారత్\\u200c వే గంగా పయనిస్తున్నది.',\n",
              " '‘విద్యార్థులను చూస్తుంటే నాకు చిన్నప్పటి రోజులు గుర్తుకొస్తున్నాయి.',\n",
              " 'ఆర్థిక లావాదేవీలన్నీ ఆన్\\u200cలైన్\\u200cలోనే',\n",
              " 'పద్మినీ స్త్రీ రాత్రి వేళల్లో రతికి ఇష్టపడదు.',\n",
              " 'ఓకే కుటుంబానికి చెందిన ముగ్గురు మృతిచెందటంతో ఈ సంఘటన కలవరపర్చింది.',\n",
              " 'ఆ సినిమాకు వి. వి. వినాయక్\\u200c దర్శకత్వం వహించనున్నారు.',\n",
              " 'హైదరాబాద్\\u200cకు మరో భారీ వర్షం సూచనలున్నాయని వాతావరణ శాఖ హెచ్చరించింది.',\n",
              " 'మనం అంటున్న మమ్ముట్టి ఫ్యామిలీ !']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length telugu: {np.percentile([len(x) for x in telugu_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )"
      ],
      "metadata": {
        "id": "0d3tx857JL8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1be0fd-dabb-40cf-9052-43d517e09b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97th percentile length telugu: 168.0\n",
            "97th percentile length English: 172.02999999999884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 200\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(telugu_sentences)):\n",
        "    telugu_sentence, english_sentence = telugu_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(telugu_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(telugu_sentence, telugu_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(telugu_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ],
      "metadata": {
        "id": "XXpC44dlKp6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb9839dc-9072-4ed9-e86d-152f6eb71d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 10000\n",
            "Number of valid sentences: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "telugu_sentences = [telugu_sentences[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
      ],
      "metadata": {
        "id": "G0Sp_ds9Kp3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telugu_sentences[:3] #first 3 telugu valid sentences"
      ],
      "metadata": {
        "id": "BVDvbG-UKptS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d44f90-22b6-4bd0-ad52-877e37f52e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['14 కోట్లు సమర్పరణ', '1 అక్టోబర్ 2001.', 'ఆత్మహత్యతో కలకలం']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "batch_size = 3\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 3 # number of encoder and decoder block so this is simple and fast\n",
        "max_sequence_length = 200\n",
        "te_vocab_size = len(telugu_vocabulary)\n",
        "\n",
        "transformer = Transformer( d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_layers,\n",
        "                          max_sequence_length,\n",
        "                          te_vocab_size,\n",
        "                          english_to_index,\n",
        "                          telugu_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN\n",
        "                         )\n"
      ],
      "metadata": {
        "id": "7rTYupWzQBwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer"
      ],
      "metadata": {
        "id": "HTcN9XVeQB8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dde6387-13b7-4215-e09c-f52928f2d895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Transformer name=transformer, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(tf.data.Dataset): #using tensorflow dataset function\n",
        "    def __new__(cls, english_sentences, telugu_sentences):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((english_sentences, telugu_sentences))\n",
        "        return dataset\n",
        "\n",
        "dataset = TextDataset(english_sentences, telugu_sentences)"
      ],
      "metadata": {
        "id": "P5IB-cJIQCRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "CS-DshRvQCVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cdd35cf-e10f-4563-a3eb-491e01d3288a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1]"
      ],
      "metadata": {
        "id": "SSwurx3gUqno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "25e97f28-3c4a-4cee-9007-3732cfdd2a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'_TensorSliceDataset' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c68bfbd5e8ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: '_TensorSliceDataset' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.batch(batch_size)  # Batching the dataset\n",
        "iterator = iter(dataset)  # Creating an iterator"
      ],
      "metadata": {
        "id": "EIdz70LPUtCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num > 3:\n",
        "        break"
      ],
      "metadata": {
        "id": "Ft3hXWwjWBhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bdd6626-85f4-4d70-f254-6486bd072004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
            "array([b'rs 14 crore', b'1 apr 2001.', b'suicide by hanging'],\n",
            "      dtype=object)>, <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
            "array([b'14 \\xe0\\xb0\\x95\\xe0\\xb1\\x8b\\xe0\\xb0\\x9f\\xe0\\xb1\\x8d\\xe0\\xb0\\xb2\\xe0\\xb1\\x81 \\xe0\\xb0\\xb8\\xe0\\xb0\\xae\\xe0\\xb0\\xb0\\xe0\\xb1\\x8d\\xe0\\xb0\\xaa\\xe0\\xb0\\xb0\\xe0\\xb0\\xa3',\n",
            "       b'1 \\xe0\\xb0\\x85\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\x9f\\xe0\\xb1\\x8b\\xe0\\xb0\\xac\\xe0\\xb0\\xb0\\xe0\\xb1\\x8d 2001.',\n",
            "       b'\\xe0\\xb0\\x86\\xe0\\xb0\\xa4\\xe0\\xb1\\x8d\\xe0\\xb0\\xae\\xe0\\xb0\\xb9\\xe0\\xb0\\xa4\\xe0\\xb1\\x8d\\xe0\\xb0\\xaf\\xe0\\xb0\\xa4\\xe0\\xb1\\x8b \\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x82'],\n",
            "      dtype=object)>)\n",
            "(<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
            "array([b'120 crores.', b'12 the bibles viewpoint', b'time is less.'],\n",
            "      dtype=object)>, <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
            "array([b'120 \\xe0\\xb0\\x95\\xe0\\xb1\\x8b\\xe0\\xb0\\x9f\\xe0\\xb1\\x8d\\xe0\\xb0\\xb2\\xe0\\xb1\\x81 \\xe0\\xb0\\x85\\xe0\\xb0\\x9f.',\n",
            "       b'12 \\xe0\\xb0\\x95\\xe0\\xb1\\x83\\xe0\\xb0\\xa4\\xe0\\xb0\\x9c\\xe0\\xb1\\x8d\\xe0\\xb0\\x9e\\xe0\\xb0\\xa4',\n",
            "       b'\\xe0\\xb0\\xb8\\xe0\\xb0\\xae\\xe0\\xb0\\xaf\\xe0\\xb0\\x82 \\xe0\\xb0\\xa4\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\x95\\xe0\\xb1\\x81\\xe0\\xb0\\xb5.'],\n",
            "      dtype=object)>)\n",
            "(<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'why is', b'rs 3.90 lakh.', b'irrfan: yes.'], dtype=object)>, <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
            "array([b'\\xe0\\xb0\\x8e\\xe0\\xb0\\x82\\xe0\\xb0\\xa6\\xe0\\xb1\\x81\\xe0\\xb0\\x95\\xe0\\xb1\\x81\\xe0\\xb0\\xa8\\xe0\\xb1\\x8d\\xe0\\xb0\\xa8\\xe0\\xb0\\x9f\\xe0\\xb1\\x8d\\xe0\\xb0\\x9f\\xe0\\xb1\\x81?',\n",
            "       b'90 \\xe0\\xb0\\xb2\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\xb7\\xe0\\xb0\\xb2\\xe0\\xb1\\x81-\\xe0\\xb0\\xb0\\xe0\\xb1\\x82.',\n",
            "       b'\\xe0\\xb0\\x8f\\xe0\\xb0\\xb0\\xe0\\xb0\\xa8\\xe0\\xb1\\x8d: \\xe0\\xb0\\x85\\xe0\\xb0\\xb5\\xe0\\xb1\\x81\\xe0\\xb0\\xa8\\xe0\\xb1\\x81.'],\n",
            "      dtype=object)>)\n",
            "(<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
            "array([b'not a single day', b'in the mens,...', b'their relatives.'],\n",
            "      dtype=object)>, <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
            "array([b'\\xe0\\xb0\\xaa\\xe0\\xb0\\x97\\xe0\\xb0\\xb2\\xe0\\xb1\\x81 \\xe0\\xb0\\xb5\\xe0\\xb0\\xa6\\xe0\\xb1\\x8d\\xe0\\xb0\\xa6\\xe0\\xb1\\x81.',\n",
            "       b'\\xe0\\xb0\\xaa\\xe0\\xb1\\x81\\xe0\\xb0\\xb0\\xe0\\xb1\\x81\\xe0\\xb0\\xb7\\xe0\\xb1\\x81\\xe0\\xb0\\xb2\\xe0\\xb0\\xb2\\xe0\\xb1\\x8b .',\n",
            "       b'\\xe0\\xb0\\xa4\\xe0\\xb0\\xae \\xe0\\xb0\\xac\\xe0\\xb0\\x82\\xe0\\xb0\\xa7\\xe0\\xb1\\x81\\xe0\\xb0\\xb5\\xe0\\xb1\\x81\\xe0\\xb0\\xb2'],\n",
            "      dtype=object)>)\n",
            "(<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'lower costs', b\"what's not\", b'reason #2'], dtype=object)>, <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
            "array([b'\\xe0\\xb0\\xa4\\xe0\\xb0\\x95\\xe0\\xb1\\x8d\\xe0\\xb0\\x95\\xe0\\xb1\\x81\\xe0\\xb0\\xb5 \\xe0\\xb0\\x96\\xe0\\xb0\\xb0\\xe0\\xb1\\x8d\\xe0\\xb0\\x9a\\xe0\\xb1\\x81\\xe0\\xb0\\xb2\\xe0\\xb1\\x81',\n",
            "       b'\\xe0\\xb0\\x8f\\xe0\\xb0\\x82 \\xe0\\xb0\\xb5\\xe0\\xb1\\x81\\xe0\\xb0\\x82\\xe0\\xb0\\xa1\\xe0\\xb0\\xa6\\xe0\\xb1\\x81.',\n",
            "       b'\\xe0\\xb0\\xaa\\xe0\\xb1\\x8d\\xe0\\xb0\\xb0\\xe0\\xb0\\xaf\\xe0\\xb1\\x8b\\xe0\\xb0\\x9c\\xe0\\xb0\\xa8\\xe0\\xb0\\x82 # 2'],\n",
            "      dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, initializers, optimizers\n",
        "\n",
        "# Define the loss function\n",
        "padding_token_index = telugu_to_index[PADDING_TOKEN]\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "# Initialize the weights of the transformer model\n",
        "for layer in transformer.layers:\n",
        "    if hasattr(layer, 'kernel'):\n",
        "        initializer = initializers.GlorotUniform()\n",
        "        layer.kernel.assign(initializer(layer.kernel.shape, dtype=layer.kernel.dtype))\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "# Set the device\n",
        "device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'"
      ],
      "metadata": {
        "id": "PAMSfsTRWZwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, kn_batch, max_sequence_length):\n",
        "    num_sentences = len(eng_batch)\n",
        "\n",
        "    # Look-ahead mask\n",
        "    look_ahead_mask = tf.linalg.band_part(tf.ones([max_sequence_length, max_sequence_length]), 0, -1)\n",
        "    look_ahead_mask = tf.cast(look_ahead_mask == 1, tf.bool)\n",
        "\n",
        "    # Padding masks\n",
        "    encoder_padding_mask = tf.fill([num_sentences, max_sequence_length, max_sequence_length], False)\n",
        "    decoder_padding_mask_self_attention = tf.fill([num_sentences, max_sequence_length, max_sequence_length], False)\n",
        "    decoder_padding_mask_cross_attention = tf.fill([num_sentences, max_sequence_length, max_sequence_length], False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "        eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "        kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "\n",
        "        # Update masks\n",
        "        encoder_padding_mask = tf.tensor_scatter_nd_update(encoder_padding_mask,\n",
        "            tf.concat([tf.expand_dims(tf.range(num_sentences), 1), tf.expand_dims(tf.range(max_sequence_length), 1), tf.expand_dims(eng_chars_to_padding_mask, 0)], axis=1),\n",
        "            tf.ones([num_sentences, max_sequence_length - len(eng_chars_to_padding_mask)], dtype=tf.bool))\n",
        "\n",
        "        encoder_padding_mask = tf.tensor_scatter_nd_update(encoder_padding_mask,\n",
        "            tf.concat([tf.expand_dims(tf.range(num_sentences), 1), tf.expand_dims(eng_chars_to_padding_mask, 1), tf.expand_dims(tf.range(max_sequence_length), 0)], axis=1),\n",
        "            tf.ones([num_sentences, max_sequence_length - len(eng_chars_to_padding_mask)], dtype=tf.bool))\n",
        "\n",
        "        decoder_padding_mask_self_attention = tf.tensor_scatter_nd_update(decoder_padding_mask_self_attention,\n",
        "            tf.concat([tf.expand_dims(tf.range(num_sentences), 1), tf.expand_dims(tf.range(max_sequence_length), 1), tf.expand_dims(kn_chars_to_padding_mask, 0)], axis=1),\n",
        "            tf.ones([num_sentences, max_sequence_length - len(kn_chars_to_padding_mask)], dtype=tf.bool))\n",
        "\n",
        "        decoder_padding_mask_self_attention = tf.tensor_scatter_nd_update(decoder_padding_mask_self_attention,\n",
        "            tf.concat([tf.expand_dims(tf.range(num_sentences), 1), tf.expand_dims(kn_chars_to_padding_mask, 1), tf.expand_dims(tf.range(max_sequence_length), 0)], axis=1),\n",
        "            tf.ones([num_sentences, max_sequence_length - len(kn_chars_to_padding_mask)], dtype=tf.bool))\n",
        "\n",
        "        decoder_padding_mask_cross_attention = tf.tensor_scatter_nd_update(decoder_padding_mask_cross_attention,\n",
        "            tf.concat([tf.expand_dims(tf.range(num_sentences), 1), tf.expand_dims(tf.range(max_sequence_length), 1), tf.expand_dims(eng_chars_to_padding_mask, 0)], axis=1),\n",
        "            tf.ones([num_sentences, max_sequence_length - len(eng_chars_to_padding_mask)], dtype=tf.bool))\n",
        "\n",
        "        decoder_padding_mask_cross_attention = tf.tensor_scatter_nd_update(decoder_padding_mask_cross_attention,\n",
        "            tf.concat([tf.expand_dims(tf.range(num_sentences), 1), tf.expand_dims(eng_chars_to_padding_mask, 1), tf.expand_dims(tf.range(max_sequence_length), 0)], axis=1),\n",
        "            tf.ones([num_sentences, max_sequence_length - len(eng_chars_to_padding_mask)], dtype=tf.bool))\n",
        "\n",
        "    # Compute masks\n",
        "    encoder_self_attention_mask = tf.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask = tf.where(look_ahead_mask | decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = tf.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"
      ],
      "metadata": {
        "id": "nKV0a-hCWZ_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "total_loss = 0\n",
        "\n",
        "# Ensure the transformer is on the correct device\n",
        "# No explicit `to(device)` is needed in TensorFlow as the model should already be on the GPU if available.\n",
        "\n",
        "# Create an optimizer (equivalent to optim in PyTorch)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function  # Compile the function for faster execution\n",
        "def train_step(eng_batch, te_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Creating masks\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)\n",
        "\n",
        "        # Forward pass\n",
        "        te_predictions = transformer(\n",
        "            eng_batch,\n",
        "            te_batch,\n",
        "            encoder_self_attention_mask,\n",
        "            decoder_self_attention_mask,\n",
        "            decoder_cross_attention_mask,\n",
        "            enc_start_token=False,\n",
        "            enc_end_token=False,\n",
        "            dec_start_token=True,\n",
        "            dec_end_token=True\n",
        "        )\n",
        "\n",
        "# Assuming `transformer.decoder.sentence_embedding.batch_tokenize` is equivalent in TensorFlow.\n",
        "labels = transformer.decoder.sentence_embedding.batch_tokenize(te_batch, start_token=False, end_token=True)\n",
        "\n",
        "# Flatten the predictions and labels for loss calculation\n",
        "te_predictions_flat = tf.reshape(te_predictions, [-1, te_vocab_size])\n",
        "labels_flat = tf.reshape(labels, [-1])\n",
        "\n",
        "# Compute the loss\n",
        "loss = tf.keras.losses.sparse_categorical_crossentropy(labels_flat, te_predictions_flat, from_logits=True)\n",
        "\n",
        "# Create a mask for valid indices (non-padding tokens)\n",
        "valid_indices = tf.not_equal(labels_flat, telugu_to_index[PADDING_TOKEN])\n",
        "\n",
        "# Apply the mask and calculate the average loss\n",
        "masked_loss = tf.reduce_sum(tf.boolean_mask(loss, valid_indices))\n",
        "mean_loss = masked_loss / tf.reduce_sum(tf.cast(valid_indices, tf.float32))\n",
        "\n",
        "# Perform backpropagation\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(transformer.trainable_variables)\n",
        "    loss_value = mean_loss\n",
        "\n",
        "grads = tape.gradient(loss_value, transformer.trainable_variables)\n",
        "optim.apply_gradients(zip(grads, transformer.trainable_variables))\n",
        "\n",
        "# Optional: Print the loss and predictions at intervals\n",
        "if batch_num % 100 == 0:\n",
        "    print(f\"Iteration {batch_num} : {mean_loss.numpy()}\")\n",
        "    print(f\"English: {eng_batch[0]}\")\n",
        "    print(f\"telugu Translation: {te_batch[0]}\")\n",
        "    te_sentence_predicted = tf.argmax(te_predictions[0], axis=1)\n",
        "\n",
        "    # Generate the predicted sentence\n",
        "    predicted_sentence = \"\"\n",
        "    for idx in te_sentence_predicted:\n",
        "        if idx == telugu_to_index[END_TOKEN]:\n",
        "            break\n",
        "        predicted_sentence += index_to_telugu[idx.numpy()]\n",
        "\n",
        "    print(f\"telugu Prediction: {predicted_sentence}\")\n",
        "\n",
        "    import tensorflow as tf\n",
        "\n",
        "# Assuming transformer is your TensorFlow model\n",
        "transformer.eval()\n",
        "te_sentence = (\"\" ,)\n",
        "eng_sentence = (\"should we go to the mall?\",)\n",
        "for word_counter in range(max_sequence_length):\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_sentence, te_sentence)\n",
        "\n",
        "    # Convert masks to TensorFlow tensors if necessary\n",
        "    encoder_self_attention_mask = tf.convert_to_tensor(encoder_self_attention_mask)\n",
        "    decoder_self_attention_mask = tf.convert_to_tensor(decoder_self_attention_mask)\n",
        "    decoder_cross_attention_mask = tf.convert_to_tensor(decoder_cross_attention_mask)\n",
        "\n",
        "    predictions = transformer(\n",
        "        eng_sentence,\n",
        "        te_sentence,\n",
        "        encoder_self_attention_mask,\n",
        "        decoder_self_attention_mask,\n",
        "        decoder_cross_attention_mask,\n",
        "        enc_start_token=False,\n",
        "        enc_end_token=False,\n",
        "        dec_start_token=True,\n",
        "        dec_end_token=False\n",
        "    )\n",
        "\n",
        "    next_token_prob_distribution = predictions[0][word_counter]\n",
        "    next_token_index = tf.argmax(next_token_prob_distribution).numpy()  # Convert to numpy to get the item\n",
        "    next_token = index_to_telugu[next_token_index]\n",
        "    te_sentence = (te_sentence[0] + next_token,)\n",
        "\n",
        "    if next_token == END_TOKEN:\n",
        "        break\n",
        "\n",
        "print(f\"Evaluation translation (should we go to the mall?) : {te_sentence}\")\n",
        "print(\"-------------------------------------------\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "eIl-hUO1_geA",
        "outputId": "87e27d26-c4dd-45c3-fa26-cda8aa6ae953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'te_batch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-706cab8c5e08>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Assuming `transformer.decoder.sentence_embedding.batch_tokenize` is equivalent in TensorFlow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Flatten the predictions and labels for loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'te_batch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "num_epochs = 10\n",
        "train_loader = iter(dataset)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    for batch_num, batch in enumerate(train_loader):\n",
        "        eng_batch, kn_batch = batch\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch, max_sequence_length)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            kn_predictions = transformer(\n",
        "                eng_batch,\n",
        "                kn_batch,\n",
        "                encoder_self_attention_mask,\n",
        "                decoder_self_attention_mask,\n",
        "                decoder_cross_attention_mask,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=True,\n",
        "                dec_end_token=True\n",
        "            )\n",
        "\n",
        "            labels = transformer.decoder.sentence_embedding.batch_tokenize(kn_batch, start_token=False, end_token=True)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            kn_predictions = tf.reshape(kn_predictions, (-1, kn_vocab_size))\n",
        "            labels = tf.reshape(labels, (-1))\n",
        "\n",
        "            valid_indices = tf.where(labels != telugu_to_index[PADDING_TOKEN])\n",
        "            valid_kn_predictions = tf.gather(kn_predictions, valid_indices)\n",
        "            valid_labels = tf.gather(labels, valid_indices)\n",
        "\n",
        "            loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=valid_kn_predictions, labels=valid_labels))\n",
        "            loss /= tf.cast(tf.size(valid_labels), tf.float32)\n",
        "            gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "            if batch_num % 100 == 0:\n",
        "              print(f\"Iteration {batch_num} : {loss.item()}\")\n",
        "              print(f\"English: {eng_batch[0]}\")\n",
        "              print(f\"telugu Translation: {kn_batch[0]}\")\n",
        "              kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)\n",
        "              predicted_sentence = \"\"\n",
        "              for idx in kn_sentence_predicted:\n",
        "                if idx == telugu_to_index[END_TOKEN]:\n",
        "                  break\n",
        "                predicted_sentence += index_to_telugu[idx.item()]\n",
        "              print(f\"telugu Prediction: {predicted_sentence}\")\n",
        "\n",
        "              transformer.eval()\n",
        "              kn_sentence = (\"\",)\n",
        "              eng_sentence = (\"should we go to the mall?\",)\n",
        "\n",
        "              for word_counter in range(max_sequence_length):\n",
        "                 encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_sentence, kn_sentence)\n",
        "\n",
        "                 # Convert masks to tensors if they aren't already\n",
        "                 encoder_self_attention_mask = tf.convert_to_tensor(encoder_self_attention_mask)\n",
        "                 decoder_self_attention_mask = tf.convert_to_tensor(decoder_self_attention_mask)\n",
        "                 decoder_cross_attention_mask = tf.convert_to_tensor(decoder_cross_attention_mask)\n",
        "\n",
        "                 predictions = transformer(\n",
        "                                           eng_sentence,\n",
        "                                           kn_sentence,\n",
        "                                           encoder_self_attention_mask,\n",
        "                                           decoder_self_attention_mask,\n",
        "                                           decoder_cross_attention_mask,\n",
        "                                           enc_start_token=False,\n",
        "                                           enc_end_token=False,\n",
        "                                           dec_start_token=True,\n",
        "                                           dec_end_token=False\n",
        "                                          )\n",
        "\n",
        "                 # Get the predicted probabilities for the next token\n",
        "                 next_token_prob_distribution = predictions[0][word_counter]\n",
        "\n",
        "                 # Find the index of the token with the highest probability\n",
        "                 next_token_index = tf.argmax(next_token_prob_distribution).numpy()  # Convert to numpy to get the integer value\n",
        "\n",
        "                 # Get the actual token from the index\n",
        "                 next_token = index_to_telugu[next_token_index]\n",
        "\n",
        "                 # Update the telugu sentence\n",
        "                 kn_sentence = (kn_sentence[0] + next_token,)\n",
        "\n",
        "                # Break if the end token is reached\n",
        "                 if next_token == END_TOKEN:\n",
        "                    break\n",
        "\n",
        "            print(f\"Evaluation translation (should we go to the mall?) : {kn_sentence}\")\n",
        "            print(\"-------------------------------------------\")"
      ],
      "metadata": {
        "id": "ck0-cDGnwm9-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "9f071a62-3f11-4694-b067-3efb4748bf8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Scalar tensor has no `len()`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-526759f07422>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0meng_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cross_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0921f7e1d72a>\u001b[0m in \u001b[0;36mcreate_masks\u001b[0;34m(eng_batch, kn_batch, max_sequence_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0meng_sentence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkn_sentence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkn_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0meng_chars_to_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_sentence_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mkn_chars_to_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkn_sentence_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;34m\"\"\"Returns the length of the first dimension in the Tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scalar tensor has no `len()`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Scalar tensor has no `len()`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "_yOWUL4Qbrb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def translate(eng_sentence):\n",
        "    eng_sentence = (eng_sentence,)\n",
        "    kn_sentence = (\"\",)\n",
        "\n",
        "    for word_counter in range(max_sequence_length):\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_sentence, kn_sentence)\n",
        "\n",
        "        predictions = transformer(\n",
        "            eng_sentence,\n",
        "            kn_sentence,\n",
        "            encoder_self_attention_mask,\n",
        "            decoder_self_attention_mask,\n",
        "            decoder_cross_attention_mask,\n",
        "            enc_start_token=False,\n",
        "            enc_end_token=False,\n",
        "            dec_start_token=True,\n",
        "            dec_end_token=False,\n",
        "            training=False  # Make sure the model is in inference mode\n",
        "        )\n",
        "\n",
        "        next_token_prob_distribution = predictions[0][word_counter]\n",
        "        next_token_index = tf.argmax(next_token_prob_distribution).numpy()\n",
        "        next_token = index_to_telugu[next_token_index]\n",
        "        kn_sentence = (kn_sentence[0] + next_token,)\n",
        "\n",
        "        if next_token == END_TOKEN:\n",
        "            break\n",
        "\n",
        "    return kn_sentence[0]"
      ],
      "metadata": {
        "id": "ZARl7-lMujej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"what should we do when the day starts?\")\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "ReOyf2jQbvjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(\"how is this the truth?\")\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "_3U_i-8jb1U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights:  ->\n",
        "When training, we can treat every alphabet as a single unit instead of splitting it into it's corresponding parts to preserve meaning. For example, ಮಾ should be 1 unit when comuting a loss. It should not be decomposed into ಮ + ఆ\n",
        "Using word-based or BPE based tokenizations may help mitigate (1). Also, we will get valid word (or BPE) units if we do so.\n",
        "Make sure the training set has a large variety of sentences that are not just about one topic like \"work\" and \"government\"\n",
        "Increase the number of encoder / decoder units for better translations. It was set to the minimum of 1 of each unit here.\n"
      ],
      "metadata": {
        "id": "eyWoKQZycLEk"
      }
    }
  ]
}